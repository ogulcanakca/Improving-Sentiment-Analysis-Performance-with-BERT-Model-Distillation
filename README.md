# Improving-Sentiment-Analysis-Performance-with-BERT-Model-Distillation
It applies the Knowledge Distillation technique to transfer the knowledge of the BERT model (Teacher) to a significantly smaller Transformer model (Student). The aim is to reduce model size and potentially inference time, while maintaining high performance or improving on standard training.
